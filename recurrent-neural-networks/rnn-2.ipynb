{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Networks with IMDb Dataset\n",
    "Build an RNN to perform text classification - predict if a review of an IMDb movie rating is positive (> 5) or negative (< 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 20000 # only grab reviews with the 'k' most frequent words; i.e. only reviews with the 20000 most frequent words are included\n",
    "max_len = 100 # Reviews all have different lengths; this will pad reviews to ensure they have the same length, to be able to be input into the RNN\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences to be of same length\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = pad_sequences(X_test, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = Sequential()\n",
    "\n",
    "'''\n",
    "Embedding layer: creates a word vector representation (assigns each word an integer)\n",
    "input_dim - No. of words\n",
    "output_dim - Embedding size\n",
    "'''\n",
    "rnn.add(Embedding(input_dim=num_words, output_dim=128, input_shape=(X_train.shape[1], )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM layer\n",
    "rnn.add(LSTM(units=128, activation='tanh'))\n",
    "\n",
    "# Output layer\n",
    "rnn.add(Dense(units=1, activation='sigmoid'))\n",
    "#rnn.add(Dropout(0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, 100, 128)          2560000   \n_________________________________________________________________\nlstm (LSTM)                  (None, 128)               131584    \n_________________________________________________________________\ndense (Dense)                (None, 1)                 129       \n=================================================================\nTotal params: 2,691,713\nTrainable params: 2,691,713\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "# Binary target variable; hence loss: binary_crossentropy\n",
    "rnn.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "rnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 25000 samples\nEpoch 1/20\n25000/25000 [==============================] - 7s 284us/sample - loss: 0.4811 - accuracy: 0.7642\nEpoch 2/20\n25000/25000 [==============================] - 4s 179us/sample - loss: 0.2931 - accuracy: 0.8820\nEpoch 3/20\n25000/25000 [==============================] - 5s 180us/sample - loss: 0.2317 - accuracy: 0.9089\nEpoch 4/20\n25000/25000 [==============================] - 5s 184us/sample - loss: 0.1887 - accuracy: 0.9291\nEpoch 5/20\n25000/25000 [==============================] - 5s 205us/sample - loss: 0.1524 - accuracy: 0.9445\nEpoch 6/20\n25000/25000 [==============================] - 5s 198us/sample - loss: 0.1246 - accuracy: 0.9559\nEpoch 7/20\n25000/25000 [==============================] - 5s 204us/sample - loss: 0.0958 - accuracy: 0.9670\nEpoch 8/20\n25000/25000 [==============================] - 5s 193us/sample - loss: 0.0766 - accuracy: 0.9748\nEpoch 9/20\n25000/25000 [==============================] - 5s 193us/sample - loss: 0.0563 - accuracy: 0.9812\nEpoch 10/20\n25000/25000 [==============================] - 5s 192us/sample - loss: 0.0410 - accuracy: 0.9872\nEpoch 11/20\n25000/25000 [==============================] - 5s 192us/sample - loss: 0.0305 - accuracy: 0.9901\nEpoch 12/20\n25000/25000 [==============================] - 5s 194us/sample - loss: 0.0219 - accuracy: 0.9933\nEpoch 13/20\n25000/25000 [==============================] - 5s 195us/sample - loss: 0.0176 - accuracy: 0.9948\nEpoch 14/20\n25000/25000 [==============================] - 5s 194us/sample - loss: 0.0140 - accuracy: 0.9962\nEpoch 15/20\n25000/25000 [==============================] - 5s 200us/sample - loss: 0.0112 - accuracy: 0.9964\nEpoch 16/20\n25000/25000 [==============================] - 5s 209us/sample - loss: 0.0088 - accuracy: 0.9972\nEpoch 17/20\n25000/25000 [==============================] - 5s 201us/sample - loss: 0.0074 - accuracy: 0.9979\nEpoch 18/20\n25000/25000 [==============================] - 5s 198us/sample - loss: 0.0061 - accuracy: 0.9983\nEpoch 19/20\n25000/25000 [==============================] - 5s 200us/sample - loss: 0.0054 - accuracy: 0.9986\nEpoch 20/20\n25000/25000 [==============================] - 5s 202us/sample - loss: 0.0051 - accuracy: 0.9982\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x1740cda0f60>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "rnn.fit(X_train, y_train, epochs=20, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "25000/25000 [==============================] - 4s 150us/sample - loss: 1.0976 - accuracy: 0.8118\nTest accuracy: 0.8118000030517578\n"
    }
   ],
   "source": [
    "test_loss, test_accuracy = rnn.evaluate(X_test, y_test)\n",
    "\n",
    "print(f'Test accuracy: {test_accuracy}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}